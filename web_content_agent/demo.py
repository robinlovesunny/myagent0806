#!/usr/bin/env python3
"""
Web Content Agent ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨ä»£ç ä¸­ä½¿ç”¨å„ä¸ªæ¨¡å—
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.crawler.web_crawler import WebCrawler
from src.processor.content_processor import ContentProcessor
from src.prompt.prompt_manager import PromptManager
from src.formatter.output_formatter import OutputFormatter, OutputFormat


def demo_crawler():
    """æ¼”ç¤ºçˆ¬è™«æ¨¡å—ä½¿ç”¨"""
    print("=== çˆ¬è™«æ¨¡å—æ¼”ç¤º ===")
    
    with WebCrawler() as crawler:
        # çˆ¬å–ç¤ºä¾‹ç½‘é¡µ
        url = "https://httpbin.org/html"
        success, content, metadata = crawler.crawl_url(url)
        
        if success:
            print(f"âœ… æˆåŠŸçˆ¬å–: {url}")
            print(f"å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"çŠ¶æ€ç : {metadata.get('status_code')}")
        else:
            print(f"âŒ çˆ¬å–å¤±è´¥: {content}")


def demo_processor():
    """æ¼”ç¤ºå†…å®¹å¤„ç†æ¨¡å—ä½¿ç”¨"""
    print("\n=== å†…å®¹å¤„ç†æ¨¡å—æ¼”ç¤º ===")
    
    # ç¤ºä¾‹HTMLå†…å®¹
    html_content = """
    <html>
        <head><title>æµ‹è¯•é¡µé¢</title></head>
        <body>
            <h1>è¿™æ˜¯æ ‡é¢˜</h1>
            <p>è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ï¼ŒåŒ…å«é‡è¦ä¿¡æ¯ã€‚</p>
            <p>è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼Œæä¾›æ›´å¤šè¯¦ç»†æè¿°ã€‚</p>
            <script>console.log('è¿™æ˜¯è„šæœ¬ï¼Œä¼šè¢«ç§»é™¤');</script>
        </body>
    </html>
    """
    
    processor = ContentProcessor()
    processed_data = processor.process_html(html_content)
    
    print(f"æå–çš„æ ‡é¢˜: {processed_data['title']}")
    print(f"ä¸»è¦å†…å®¹é•¿åº¦: {len(processed_data['main_content'])}")
    print(f"å†…å®¹æ‘˜è¦: {processed_data['summary']}")


def demo_prompt_manager():
    """æ¼”ç¤ºæç¤ºè¯ç®¡ç†æ¨¡å—ä½¿ç”¨"""
    print("\n=== æç¤ºè¯ç®¡ç†æ¨¡å—æ¼”ç¤º ===")
    
    manager = PromptManager()
    
    # åˆ—å‡ºæ‰€æœ‰æ¨¡æ¿
    templates = manager.list_templates()
    print("å¯ç”¨æ¨¡æ¿:")
    for template in templates:
        print(f"  - {template['name']}: {template['description']}")
    
    # æ ¼å¼åŒ–æç¤ºè¯
    test_content = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«ä¸€äº›é‡è¦ä¿¡æ¯ã€‚"
    system_prompt, user_prompt = manager.format_prompts("xiaohongshu", test_content)
    
    print(f"\nä½¿ç”¨å°çº¢ä¹¦æ¨¡æ¿æ ¼å¼åŒ–çš„æç¤ºè¯:")
    print(f"ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
    print(f"ç”¨æˆ·æç¤ºè¯: {user_prompt[:100]}...")


def demo_formatter():
    """æ¼”ç¤ºè¾“å‡ºæ ¼å¼åŒ–æ¨¡å—ä½¿ç”¨"""
    print("\n=== è¾“å‡ºæ ¼å¼åŒ–æ¨¡å—æ¼”ç¤º ===")
    
    formatter = OutputFormatter()
    test_content = """
# æµ‹è¯•æ ‡é¢˜

è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å†…å®¹ï¼Œç”¨äºæ¼”ç¤ºæ ¼å¼åŒ–åŠŸèƒ½ã€‚

## ä¸»è¦ç‰¹ç‚¹

- åŠŸèƒ½å¼ºå¤§ ğŸ’ª
- æ˜“äºä½¿ç”¨ âœ¨
- å¼€æºå…è´¹ ğŸ‰

å¸Œæœ›è¿™ä¸ªå·¥å…·å¯¹ä½ æœ‰å¸®åŠ©ï¼
    """
    
    metadata = {
        'source_url': 'https://example.com',
        'template': 'xiaohongshu',
        'timestamp': '2024-01-01 12:00:00'
    }
    
    # æµ‹è¯•ä¸åŒæ ¼å¼
    formats = [
        (OutputFormat.MARKDOWN, "Markdown"),
        (OutputFormat.HTML, "HTML"),
        (OutputFormat.TEXT, "çº¯æ–‡æœ¬"),
        (OutputFormat.JSON, "JSON")
    ]
    
    for format_type, format_name in formats:
        formatted = formatter.format_content(test_content, format_type, metadata)
        print(f"\n{format_name}æ ¼å¼è¾“å‡ºé•¿åº¦: {len(formatted)} å­—ç¬¦")
        if format_type == OutputFormat.TEXT:
            print("çº¯æ–‡æœ¬æ ¼å¼é¢„è§ˆ:")
            print(formatted[:200] + "..." if len(formatted) > 200 else formatted)


def demo_full_workflow():
    """æ¼”ç¤ºå®Œæ•´å·¥ä½œæµç¨‹"""
    print("\n=== å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º ===")
    
    # 1. çˆ¬å–å†…å®¹
    url = "https://httpbin.org/html"
    print(f"1. çˆ¬å–ç½‘é¡µ: {url}")
    
    with WebCrawler() as crawler:
        success, html_content, crawl_metadata = crawler.crawl_url(url)
    
    if not success:
        print(f"çˆ¬å–å¤±è´¥: {html_content}")
        return
    
    # 2. å¤„ç†å†…å®¹
    print("2. å¤„ç†ç½‘é¡µå†…å®¹...")
    processor = ContentProcessor()
    readable_content = processor.extract_readable_content(html_content, url)
    
    # 3. æ ¼å¼åŒ–æç¤ºè¯
    print("3. æ ¼å¼åŒ–æç¤ºè¯...")
    manager = PromptManager()
    system_prompt, user_prompt = manager.format_prompts("summary", readable_content)
    
    # 4. æ ¼å¼åŒ–è¾“å‡ºï¼ˆæ¨¡æ‹ŸAIç”Ÿæˆçš„å†…å®¹ï¼‰
    print("4. æ ¼å¼åŒ–è¾“å‡º...")
    ai_generated_content = "è¿™æ˜¯ä¸€ä¸ªHTMLæµ‹è¯•é¡µé¢çš„æ‘˜è¦ã€‚é¡µé¢åŒ…å«äº†åŸºæœ¬çš„HTMLç»“æ„ï¼Œä¸»è¦ç”¨äºæµ‹è¯•ç½‘é¡µçˆ¬å–å’Œå¤„ç†åŠŸèƒ½ã€‚"
    
    formatter = OutputFormatter()
    metadata = formatter.add_metadata(
        content=ai_generated_content,
        source_url=url,
        template="summary"
    )
    
    final_output = formatter.format_content(ai_generated_content, OutputFormat.MARKDOWN, metadata)
    
    print("âœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆï¼")
    print("æœ€ç»ˆè¾“å‡º:")
    print("-" * 50)
    print(final_output)


if __name__ == "__main__":
    print("Web Content Agent åŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    try:
        demo_crawler()
        demo_processor()
        demo_prompt_manager()
        demo_formatter()
        demo_full_workflow()
        
        print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nè¦åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œè¯·:")
        print("1. è®¾ç½®é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥")
        print("2. ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·: python main.py --api-key YOUR_KEY --url TARGET_URL")
        print("3. æˆ–åœ¨ä»£ç ä¸­å¯¼å…¥WebContentAgentç±»è¿›è¡Œä½¿ç”¨")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()